# Neural Network
## 神经元模型
        神经网络是由具有适应性的简单单元组成的广泛并行互连的网络, 它的组织能够
    模拟生物神经系统对真实世界物体所作出的交互反应

        神经网络中最基本的成分是神经元(neuron)模型, 即上述定义中的"简单单元".
    在生物神经网络中, 每个神经元与其它神经元相连, 当它"兴奋"时, 就会向相连的
    神经元发送化学物质, 从而改变这些神经元内的电位; 如果某神经元的点位超过了一个
    "阈值"(threshold), 那么它就会被激活, 即"兴奋"起来, 向其它神经元发送化学物
    质
[![M-P Neuron Model](https://i.loli.net/2018/07/22/5b5481fb7dfcd.jpg)](https://i.loli.net/2018/07/22/5b5481fb7dfcd.jpg)
## 感知机(Perception)与多层网络
    感知机由两层神经元组成, 输入层接收外界输入信号后传递给输出层, 输出层是M-P神经元, 亦称
    "阈值逻辑单元"(threshold logic unit)
    感知机只有输出层神经元进行激活函数处理, 即只拥有一层功能神经元(functional neuron),
    其学习能力有限.
    若两类模式是线性可分的, 即存在一个线性超平面能将它们分开;
    则感知机的学习过程一定会收敛(converge), 而求得适当的权向量W=(w1;w2;...;wn+1);
    否则, 感知机学习过程将会发生振荡(fluctuation), W难以稳定下来, 不能求得合适解
    多层功能神经元, 可以解决非线性可分问题
- 多层前馈神经网络(multi-layer feed-forward neural network)<br/>
每层神经元与下一层神经元全互连, 神经元之间不存在同层连接, 也不存在跨层连接
## 误差逆传播算法(error BackPropagation)
    BP是基于梯度下降(gradient descent)策略, 以目标的负梯度方向对参数进行调整.
## 全局最小(global minimum)和局部最小(local minimum)
    跳出"局部最小"的策略
- 使用随机梯度下降
- 使用模拟退火(simulated annealing)
- 以多组不同参数值初始化多个神经网络, 按标准方法训练后, 取其中误差最小的解作为最终参数
- 使用遗传算法(genetic algorithms)
## 其他常见神经网络
- RBF(Radial Basis Function, 径向基函数)网络<br/>
    一种单隐层前馈神经网络, 它使用径向基函数作为隐层神经元激活函数, <br/>
    而输出层则是对隐层神经元输出的线性组合
- ART(Adaptive Resonance Theory, 自适应谐振理论)网络<br/>
    竞争型学习(competitive learning)是神经网络中一种常用的无监督学习策略, 在使用该策略时, <br/>
    网络的输出神经元相互竞争, 每一时刻仅有一个竞争获胜的神经元被激活, 其他神经元的状态被抑制. <br/>
    这种机制亦称"胜者通吃(winner-take-all)"原则<br/>
    ART是竞争学习的重要代表, 可以好地缓解了竞争型学习中的"可塑性-稳定性窘境(stability-plasticity dilemma)"<br/>
    可塑性是指神经网络要有学习新知识的能力, 而稳定性则是指神经网络在学习新知识时要保持对旧知识的记忆.<br/>
    这就使得ART网络具有一个很重要的优点: 可进行增量学习(incremental learning)或在线学习(online learning)
- SOM(Self-Organizing Map, 自组织映射)网络<br/>
    是一种竞争学习型的无监督神经网络, 它能将高位输入数据映射到低维空间(通常是二维), 同时保持输入数据在高维<br/>
    空间的拓扑结构, 即将高维空间中相似的样本点映射到网络输出层中的邻近神经元.
- 级联相关(Cascade-Correlation)网络<br/>
- Elman网络<br/>
    与前馈神经网络不同, "递归神经网络(recurrent neural network)"允许网络中出现环形结构, 从而可让<br/>
    一些神经元的输出反馈回来作为输入信号
- Boltzmann机<br/>
    一种"基于能量的模型(energy-based model)"<br/>
    其神经元都是布尔型
## 深度学习
        理论上来说, 参数越多的模型复杂度越高, "容量(capacity)"越大, 这意味着它能完成更复杂的学习任务.
    但一般情况下, 复杂模型的训练效率低, 易陷于过拟合, 因此难以收到青睐.
    而随着云计算、大数据时代的到来, 计算能力的大幅提高可缓解悬链低效性, 训练数据的大幅增加则可降低过拟合风险
        多隐层, 是指三个以上隐层; 深度学习模型通常啊有八九层甚至更多的隐层
    多隐层神经网络难以直接用经典算法(如, 标准BP算法)进行训练, 因为误差在多隐层内逆传播时, 往往会"发散(diverge)"
    而不能收敛到稳定状态
        无监督逐层训练(unsupervised layer-wise training)是多隐层网络训练的有效手段
    其基本思想是每次训练一层隐结点, 训练时将上一层隐结点的输出作为输入, 而本层隐结点的输出作为下一层隐结点的输入,
    这称为"预训练(pre-training)", 在预训练全部完成后, 再对整个网络进行"微调"(fine-tuning)训练
        事实上, "预训练+微调"的做法可视为将大量参数分组, 对每组先找到局部看来比较好的位置, 然后再基于这些局部
    最优的结果联合起来进行全局寻优. 这样就在利用了模型大量参数所提供的自由度的同时, 有效的节省了训练开销
        另一种节省训练开销的策略是"权共享(weight sharing)", 即让一组神经元使用相同的连接权.



